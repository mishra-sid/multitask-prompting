# -*- coding: utf-8 -*-
"""bert-base

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KoYD0Gtvx8uk6wPYdrfGAwf4b2o7IVVZ
"""

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-cased')


import torch
from datasets import load_dataset
from datasets.arrow_dataset import concatenate_datasets
from transformers import AdamW, AutoModelForSequenceClassification
from transformers import AutoTokenizer,DataCollatorWithPadding
from transformers import TrainingArguments
from transformers import Trainer


data_set = load_dataset("nlu_evaluation_data",split='train')

data_alarm_audio= data_set.filter(lambda example: example['scenario'] == 'alarm' or example['scenario'] == 'audio')

data_alarm_audio

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")



def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = data_alarm_audio.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

tt_set = tokenized_datasets.train_test_split(test_size=0.3)

# from torch.utils.data import DataLoader

# train_dataloader = DataLoader(
#     tt_set["train"], shuffle=True, batch_size=8, collate_fn=data_collator
# )
# eval_dataloader = DataLoader(
#     tt_set["test"], batch_size=8, collate_fn=data_collator
# )

tt_set

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=7)


# training_args = TrainingArguments("test_trainer")
training_args = TrainingArguments(output_dir = "~",per_device_train_batch_size=2,save_steps=20,report_to="wandb")


trainer = Trainer(model=model, args=training_args, train_dataset=tt_set["train"], eval_dataset=tt_set["test"])

trainer.train()
