{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "diverse-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nlu_evaluation_data (/home/sid/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282)\n",
      "Loading cached shuffled indices for dataset at /home/sid/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-ea2cd470d1f78acb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_set = load_dataset(\"nlu_evaluation_data\",split='train')\n",
    "data_set = data_set.shuffle(seed=42)\n",
    "\n",
    "labels = data_set.features[\"label\"].names\n",
    "num_labels = len(labels)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proud-scientist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f907de96b5742589cf091172e9f2703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33a0347d1c24658af6b636e00c342cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25715 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=64, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "data_set = data_set.map(tokenize_function)\n",
    "data_set = data_set.rename_column(\"label\", \"labels\")\n",
    "data_set.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_valid_test = data_set.train_test_split(test_size=0.3)\n",
    "train_valid = train_valid_test['train'].train_test_split(test_size=0.3)\n",
    "\n",
    "train_data =  train_valid['train']\n",
    "valid_data = train_valid['test']\n",
    "test_data = train_valid_test['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adequate-expert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5892d8d1a2942f1aa069d919f02c9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cooperative-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_data)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=16)\n",
    "eval_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "infectious-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def evaluate():\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        gbatch = {}\n",
    "        for k, v in batch.items():\n",
    "            if k in [\"input_ids\", \"attention_mask\"]:\n",
    "                gbatch[k] = v[0].to(device)\n",
    "            elif k == \"labels\":\n",
    "                gbatch[k] = v.to(device) \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**gbatch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "favorite-queensland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b29baf055249fcb7362fe1ec0d8137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/252000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.01 GiB already allocated; 0 bytes free; 5.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sid/Research/multitask-prompting/notebooks/nlu-eval-bert.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sid/Research/multitask-prompting/notebooks/nlu-eval-bert.ipynb#ch0000005vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39melif\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sid/Research/multitask-prompting/notebooks/nlu-eval-bert.ipynb#ch0000005vscode-remote?line=13'>14</a>\u001b[0m         gbatch[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sid/Research/multitask-prompting/notebooks/nlu-eval-bert.ipynb#ch0000005vscode-remote?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sid/Research/multitask-prompting/notebooks/nlu-eval-bert.ipynb#ch0000005vscode-remote?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sid/Research/multitask-prompting/notebooks/nlu-eval-bert.ipynb#ch0000005vscode-remote?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1204\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1195'>1196</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1196'>1197</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1197'>1198</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1198'>1199</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1199'>1200</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1200'>1201</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1201'>1202</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1203'>1204</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1204'>1205</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1205'>1206</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1206'>1207</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1207'>1208</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1208'>1209</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1209'>1210</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1210'>1211</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1211'>1212</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1212'>1213</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1213'>1214</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1214'>1215</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1215'>1216</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:850\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=840'>841</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=842'>843</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=843'>844</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=844'>845</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=847'>848</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=848'>849</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=849'>850</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=850'>851</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=851'>852</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=852'>853</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=853'>854</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=854'>855</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=855'>856</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=856'>857</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=857'>858</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=858'>859</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=859'>860</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=860'>861</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=861'>862</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=862'>863</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=516'>517</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=517'>518</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=518'>519</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=522'>523</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=523'>524</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=524'>525</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=525'>526</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=526'>527</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=527'>528</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=528'>529</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=529'>530</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=530'>531</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=531'>532</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=532'>533</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=533'>534</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=535'>536</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=536'>537</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=399'>400</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=400'>401</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=401'>402</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=408'>409</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=409'>410</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=410'>411</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=411'>412</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=412'>413</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=413'>414</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=414'>415</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=415'>416</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=416'>417</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=417'>418</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=418'>419</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=420'>421</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=328'>329</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=329'>330</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=330'>331</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=336'>337</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=337'>338</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=338'>339</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=339'>340</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=340'>341</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=341'>342</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=342'>343</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=343'>344</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=344'>345</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=345'>346</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=346'>347</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=347'>348</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=348'>349</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:269\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=264'>265</a>\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=266'>267</a>\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=267'>268</a>\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=268'>269</a>\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=270'>271</a>\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=271'>272</a>\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/dropout.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/dropout.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/functional.py:1169\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/functional.py?line=1166'>1167</a>\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/functional.py?line=1167'>1168</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> <a href='file:///home/sid/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/functional.py?line=1168'>1169</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.01 GiB already allocated; 0 bytes free; 5.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_yet = 0.0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        gbatch = {}\n",
    "        for k, v in batch.items():\n",
    "            if k in [\"input_ids\", \"attention_mask\"]:\n",
    "                gbatch[k] = v[0].to(device)\n",
    "            elif k == \"labels\":\n",
    "                gbatch[k] = v.to(device) \n",
    "        outputs = model(**gbatch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    metrics = evaluate()\n",
    "    print(f\"epoch {epoch} metrics {metrics}\")\n",
    "    if metrics['accuracy'] > best_yet:\n",
    "        test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-celtic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b9a2d448f6f5570c572280912f28598c5037f393e09d0c1c0b04c8b3d79c852"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
